InputEmbedding:
    The goal of this class is to convert token indices into dense embedding vectors, 
    scaled by the dimension of the model (d_model). These embeddings are used as the initial input to the transformer model, 
    encoding information about the token’s position in the vocabulary.

PositionEncoding:
    The goal of this class is to inject positional information into the token embeddings using sine and cosine functions. 
    This helps the transformer model understand the order of tokens,
    which is critical for tasks involving sequential data like translation. 
    The dropout layer helps improve generalization.


Layer Norm:
    Layer Normalization plays a critical role in transformers, 
    particularly in the residual connections between layers. 
    In transformer architectures:

        1> The output of sub-layers (e.g., multi-head attention or feedforward layers) is added back to the original input.
        2> Before this addition (or sometimes after), 
        layer normalization ensures that the model doesn’t face issues like vanishing or exploding gradients, 
        and that the range of values passed between layers remains controlled.

Feedforward:
    The purpose of this block is to enhance the expressiveness of the transformer model by adding a simple two-layer feedforward network after the attention mechanism. 
    This block is responsible for learning complex patterns by introducing non-linear transformations and then projecting back to the original feature space (d_model).

MultiHeadAttention:
    Initialization:
        Set up the linear transformations (for query, key, value, and output) and define the number of heads.
    Attention Function:
        Compute attention scores (dot-product, scaling, softmax) and apply them to the value vectors.
    Forward Pass:
        1> Transform the input into query, key, and value vectors.
        2> Split into multiple heads, compute attention for each head, and concatenate the results.
        3> Apply the final linear transformation to get the output.

residualConnection or skip connection:
    The goal of this class is to implement a residual connection (or "skip connection") around a sublayer (e.g., multi-head attention or feed-forward layer) along with dropout and layer normalization. 
    Residual connections help prevent the degradation of signals during training, 
    which is critical for deep models like transformers.

encoderblock:
    The EncoderBlock is designed to process an input sequence through self-attention and a feed-forward network while maintaining residual connections. 
    This allows the encoder to capture both local and global dependencies in the input sequence, 
    which is crucial for tasks like translation.

encoder:
    The Encoder class represents a stack of EncoderBlock layers, 
    each containing self-attention and feed-forward blocks with residual connections. 
    This class iterates over the multiple layers and normalizes the output at the end. 
    The main purpose of the encoder is to extract features and patterns from the input sequence in a way that preserves information across different levels of abstraction.

decoderBlock:
    The DecoderBlock is a core component of the decoder in the Transformer model. 
    Its goal is to process the target sequence (the sequence that we want to generate or predict) while considering both the target sequence (via self-attention) and the input sequence (via cross-attention). 
    This allows the model to generate the next token in a sequence by learning from both what has already been generated (target) and the context of the input (encoder output).

decoder:
    This Decoder class works in tandem with the Encoder to generate outputs in tasks like machine translation, 
    where the model needs to both attend to the source sentence (via cross-attention) and generate the target sentence sequentially.

